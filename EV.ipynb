{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6906ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `hdfs://localhost:9000/user/ashok/data_files/EasyVisa/EasyVisa.csv': File exists\n",
      "Found 1 items\n",
      "-rw-r--r--   1 ashok supergroup      1.8 M 2023-10-14 06:30 hdfs://localhost:9000/user/ashok/data_files/EasyVisa/EasyVisa.csv\n"
     ]
    }
   ],
   "source": [
    "! cd ~\n",
    "! hdfs dfs -mkdir -p  hdfs://localhost:9000/user/ashok/data_files/EasyVisa\n",
    "! hdfs dfs -put /home/ashok/Downloads/EasyVisa.csv hdfs://localhost:9000/user/ashok/data_files/EasyVisa\n",
    "! hdfs dfs -put /home/ashok/Downloads/EasyVisa.csv hdfs://localhost:9000/user/ashok/data_files/EasyVisa\n",
    "! hdfs dfs -ls -h hdfs://localhost:9000/user/ashok/data_files/EasyVisa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d94a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1.1   For transforming categorical data to integer and to dummy\n",
    "from pyspark.ml.feature import  StringIndexer, OneHotEncoder\n",
    "\n",
    "# 1.2   For collecting all features at one place\n",
    "#       A feature transformer that merges multiple columns into one vector column.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 1.3 Minmax scaler\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "\n",
    "# 1.4   To execute all transformation operations as pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1.5 Logistic Regression modeling\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 1.6 Parameter grid builder and cross-validator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# 1.7 Evaluate classification. Default AUC.\n",
    "#     Change parameter 'metricName' for others\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# 1.8 \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1.9 Misc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1658aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fc0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e43c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4846/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07e87a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|case_id|continent|education_of_employee|has_job_experience|requires_job_training|no_of_employees|yr_of_estab|region_of_employment|prevailing_wage|unit_of_wage|full_time_position|case_status|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "| EZYV01|     Asia|          High School|                 N|                    N|          14513|       2007|                West|       592.2029|        Hour|                 Y|     Denied|\n",
      "| EZYV02|     Asia|             Master's|                 Y|                    N|           2412|       2002|           Northeast|       83425.65|        Year|                 Y|  Certified|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "                      path = \"/user/ashok/data_files/EasyVisa\",\n",
    "                      sep = \",\",\n",
    "                      inferSchema = True,\n",
    "                      ignoreLeadingWhiteSpace = True,\n",
    "                      ignoreTrailingWhiteSpace= True,\n",
    "                      header = True,\n",
    "                      nullValue = \"?\"           \n",
    "                      \n",
    "                      )\n",
    "\n",
    "df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f216c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'continent',\n",
       " 'education_of_employee',\n",
       " 'has_job_experience',\n",
       " 'requires_job_training',\n",
       " 'no_of_employees',\n",
       " 'yr_of_estab',\n",
       " 'region_of_employment',\n",
       " 'prevailing_wage',\n",
       " 'unit_of_wage',\n",
       " 'full_time_position',\n",
       " 'case_status']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('case_id', 'string'),\n",
       " ('continent', 'string'),\n",
       " ('education_of_employee', 'string'),\n",
       " ('has_job_experience', 'string'),\n",
       " ('requires_job_training', 'string'),\n",
       " ('no_of_employees', 'int'),\n",
       " ('yr_of_estab', 'int'),\n",
       " ('region_of_employment', 'string'),\n",
       " ('prevailing_wage', 'double'),\n",
       " ('unit_of_wage', 'string'),\n",
       " ('full_time_position', 'string'),\n",
       " ('case_status', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save df's initial column list\n",
    "cols_original = df.columns\n",
    "cols_original\n",
    "\n",
    "\n",
    "# DataFrame dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6829b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|case_status|\n",
      "+-----------+\n",
      "|     Denied|\n",
      "|  Certified|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('case_status').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0032480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25480"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db297706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|case_status|count|\n",
      "+-----------+-----+\n",
      "|     Denied| 8462|\n",
      "|  Certified|17018|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dx = df.groupby('case_status').count()\n",
    "dx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e74cb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per column how many null values:\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Create function to discover null values:\n",
    "\n",
    "def null_values(data):\n",
    "  data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5409d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------------------+-------------------------+----------------------------+----------------------+------------------+---------------------------+----------------------+-------------------+-------------------------+------------------+\n",
      "|isnan(case_id)|isnan(continent)|isnan(education_of_employee)|isnan(has_job_experience)|isnan(requires_job_training)|isnan(no_of_employees)|isnan(yr_of_estab)|isnan(region_of_employment)|isnan(prevailing_wage)|isnan(unit_of_wage)|isnan(full_time_position)|isnan(case_status)|\n",
      "+--------------+----------------+----------------------------+-------------------------+----------------------------+----------------------+------------------+---------------------------+----------------------+-------------------+-------------------------+------------------+\n",
      "|         false|           false|                       false|                    false|                       false|                 false|             false|                      false|                 false|              false|                    false|             false|\n",
      "|         false|           false|                       false|                    false|                       false|                 false|             false|                      false|                 false|              false|                    false|             false|\n",
      "+--------------+----------------+----------------------------+-------------------------+----------------------------+----------------------+------------------+---------------------------+----------------------+-------------------+-------------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------------------------+-----------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+---------------------------------------------+---------------------------------------------------------------+-----------------------------------------------------+-----------------------------------------------+-----------------------------------------------------------+---------------------------------------------+\n",
      "|(isnan(case_id) OR (case_id IS NULL))|(isnan(continent) OR (continent IS NULL))|(isnan(education_of_employee) OR (education_of_employee IS NULL))|(isnan(has_job_experience) OR (has_job_experience IS NULL))|(isnan(requires_job_training) OR (requires_job_training IS NULL))|(isnan(no_of_employees) OR (no_of_employees IS NULL))|(isnan(yr_of_estab) OR (yr_of_estab IS NULL))|(isnan(region_of_employment) OR (region_of_employment IS NULL))|(isnan(prevailing_wage) OR (prevailing_wage IS NULL))|(isnan(unit_of_wage) OR (unit_of_wage IS NULL))|(isnan(full_time_position) OR (full_time_position IS NULL))|(isnan(case_status) OR (case_status IS NULL))|\n",
      "+-------------------------------------+-----------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+---------------------------------------------+---------------------------------------------------------------+-----------------------------------------------------+-----------------------------------------------+-----------------------------------------------------------+---------------------------------------------+\n",
      "|                                false|                                    false|                                                            false|                                                      false|                                                            false|                                                false|                                        false|                                                          false|                                                false|                                          false|                                                      false|                                        false|\n",
      "|                                false|                                    false|                                                            false|                                                      false|                                                            false|                                                false|                                        false|                                                          false|                                                false|                                          false|                                                      false|                                        false|\n",
      "+-------------------------------------+-----------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+---------------------------------------------+---------------------------------------------------------------+-----------------------------------------------------+-----------------------------------------------+-----------------------------------------------------------+---------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|case_id|continent|education_of_employee|has_job_experience|requires_job_training|no_of_employees|yr_of_estab|region_of_employment|prevailing_wage|unit_of_wage|full_time_position|case_status|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|  false|    false|                false|             false|                false|          false|      false|               false|          false|       false|             false|      false|\n",
      "|  false|    false|                false|             false|                false|          false|      false|               false|          false|       false|             false|      false|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|CASE WHEN (isnan(case_id) OR (case_id IS NULL)) THEN case_id END|CASE WHEN (isnan(continent) OR (continent IS NULL)) THEN continent END|CASE WHEN (isnan(education_of_employee) OR (education_of_employee IS NULL)) THEN education_of_employee END|CASE WHEN (isnan(has_job_experience) OR (has_job_experience IS NULL)) THEN has_job_experience END|CASE WHEN (isnan(requires_job_training) OR (requires_job_training IS NULL)) THEN requires_job_training END|CASE WHEN (isnan(no_of_employees) OR (no_of_employees IS NULL)) THEN no_of_employees END|CASE WHEN (isnan(yr_of_estab) OR (yr_of_estab IS NULL)) THEN yr_of_estab END|CASE WHEN (isnan(region_of_employment) OR (region_of_employment IS NULL)) THEN region_of_employment END|CASE WHEN (isnan(prevailing_wage) OR (prevailing_wage IS NULL)) THEN prevailing_wage END|CASE WHEN (isnan(unit_of_wage) OR (unit_of_wage IS NULL)) THEN unit_of_wage END|CASE WHEN (isnan(full_time_position) OR (full_time_position IS NULL)) THEN full_time_position END|CASE WHEN (isnan(case_status) OR (case_status IS NULL)) THEN case_status END|\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|                                                            null|                                                                  null|                                                                                                      null|                                                                                             null|                                                                                                      null|                                                                                    null|                                                                        null|                                                                                                   null|                                                                                    null|                                                                           null|                                                                                             null|                                                                        null|\n",
      "|                                                            null|                                                                  null|                                                                                                      null|                                                                                             null|                                                                                                      null|                                                                                    null|                                                                        null|                                                                                                   null|                                                                                    null|                                                                           null|                                                                                             null|                                                                        null|\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|count(CASE WHEN (isnan(case_id) OR (case_id IS NULL)) THEN case_id END)|count(CASE WHEN (isnan(continent) OR (continent IS NULL)) THEN continent END)|count(CASE WHEN (isnan(education_of_employee) OR (education_of_employee IS NULL)) THEN education_of_employee END)|count(CASE WHEN (isnan(has_job_experience) OR (has_job_experience IS NULL)) THEN has_job_experience END)|count(CASE WHEN (isnan(requires_job_training) OR (requires_job_training IS NULL)) THEN requires_job_training END)|count(CASE WHEN (isnan(no_of_employees) OR (no_of_employees IS NULL)) THEN no_of_employees END)|count(CASE WHEN (isnan(yr_of_estab) OR (yr_of_estab IS NULL)) THEN yr_of_estab END)|count(CASE WHEN (isnan(region_of_employment) OR (region_of_employment IS NULL)) THEN region_of_employment END)|count(CASE WHEN (isnan(prevailing_wage) OR (prevailing_wage IS NULL)) THEN prevailing_wage END)|count(CASE WHEN (isnan(unit_of_wage) OR (unit_of_wage IS NULL)) THEN unit_of_wage END)|count(CASE WHEN (isnan(full_time_position) OR (full_time_position IS NULL)) THEN full_time_position END)|count(CASE WHEN (isnan(case_status) OR (case_status IS NULL)) THEN case_status END)|\n",
      "+-----------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "|                                                                      0|                                                                            0|                                                                                                                0|                                                                                                       0|                                                                                                                0|                                                                                              0|                                                                                  0|                                                                                                             0|                                                                                              0|                                                                                     0|                                                                                                       0|                                                                                  0|\n",
      "+-----------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understanding the above function--step-by-step\n",
    "# a\n",
    "df.select([isnan(c)  for c in df.columns]).show(2)\n",
    "# b\n",
    "df.select([ (isnan(c)|  col(c).isNull()) for c in df.columns]).show(2)\n",
    "# c\n",
    "df.select([ (isnan(c)|  col(c).isNull()).alias(c)  for c in df.columns]).show(2)\n",
    "# d\n",
    "df.select([ when ( ( isnan(c) |  col(c).isNull()), c)  for c in df.columns]).show(2)\n",
    "# e\n",
    "df.select([ count(when ( ( isnan(c) |  col(c).isNull()), c))  for c in df.columns]).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c54a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|case_id|continent|education_of_employee|has_job_experience|requires_job_training|no_of_employees|yr_of_estab|region_of_employment|prevailing_wage|unit_of_wage|full_time_position|case_status|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|      0|        0|                    0|                 0|                    0|              0|          0|                   0|              0|           0|                 0|          0|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b60dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use where filter and isNull()\n",
    "df.select('*').where(df.case_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce9bb441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25480"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('*').where(~df.case_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05c3be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'string'),\n",
       " ('continent', 'string'),\n",
       " ('education_of_employee', 'string'),\n",
       " ('has_job_experience', 'string'),\n",
       " ('requires_job_training', 'string'),\n",
       " ('no_of_employees', 'int'),\n",
       " ('yr_of_estab', 'int'),\n",
       " ('region_of_employment', 'string'),\n",
       " ('prevailing_wage', 'double'),\n",
       " ('unit_of_wage', 'string'),\n",
       " ('full_time_position', 'string'),\n",
       " ('case_status', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODELLING PIPELINE\n",
    "x = df.dtypes\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6194fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'continent',\n",
       " 'education_of_employee',\n",
       " 'has_job_experience',\n",
       " 'requires_job_training',\n",
       " 'region_of_employment',\n",
       " 'unit_of_wage',\n",
       " 'full_time_position',\n",
       " 'case_status']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are my column names:\n",
    "\n",
    "cat_cols= [ c[0]  for c in x if c[1] == \"string\" ]\n",
    "cat_cols      \n",
    "\n",
    "len(cat_cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "febe08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols= cat_cols[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b898a475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no_of_employees', 'yr_of_estab', 'prevailing_wage']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = [ i[0]   for i in df.dtypes  if i[1] != \"string\" ]\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc871a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id_index',\n",
       " 'continent_index',\n",
       " 'education_of_employee_index',\n",
       " 'has_job_experience_index',\n",
       " 'requires_job_training_index',\n",
       " 'region_of_employment_index',\n",
       " 'unit_of_wage_index',\n",
       " 'full_time_position_index',\n",
       " 'case_status_index']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After String indexing, names of our\n",
    "#        cat_columns would be:\n",
    "\n",
    "cat_cols_si = [c+\"_index\"  for c in cat_cols]\n",
    "cat_cols_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a6e1db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id_ohe',\n",
       " 'continent_ohe',\n",
       " 'education_of_employee_ohe',\n",
       " 'has_job_experience_ohe',\n",
       " 'requires_job_training_ohe',\n",
       " 'region_of_employment_ohe',\n",
       " 'unit_of_wage_ohe',\n",
       " 'full_time_position_ohe',\n",
       " 'case_status_ohe']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_cols = [ c + \"_ohe\"   for  c  in cat_cols ]\n",
    "ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3f014f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    continent|count|\n",
      "+-------------+-----+\n",
      "|       Europe| 3732|\n",
      "|       Africa|  551|\n",
      "|North America| 3292|\n",
      "+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|    continent|count|\n",
      "+-------------+-----+\n",
      "|      Oceania|  192|\n",
      "|       Africa|  551|\n",
      "|South America|  852|\n",
      "+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|    continent|count|\n",
      "+-------------+-----+\n",
      "|         Asia|16861|\n",
      "|       Europe| 3732|\n",
      "|North America| 3292|\n",
      "+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(continent='Asia', count=16861)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting mode of a feature--step-by-step\n",
    "\n",
    "df.groupby('continent').count().show(3)\n",
    "df.groupby('continent').count().orderBy(\"count\").show(3)\n",
    "df.groupby('continent').count().orderBy(\"count\",ascending = False).show(3)\n",
    "df.groupby('continent').count().orderBy(\"count\",ascending = False).first()   \n",
    "\n",
    "# Row object:\n",
    "df.groupby('continent').count().orderBy(\"count\",ascending = False).first()['continent'] \n",
    "df.groupby('continent').count().orderBy(\"count\",ascending = False).first().continent    \n",
    "df.groupby('continent').count().orderBy(\"count\",ascending = False).first()[0]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c985e2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['case_id', 'EZYV888'],\n",
       " ['continent', 'Asia'],\n",
       " ['education_of_employee', \"Bachelor's\"],\n",
       " ['has_job_experience', 'Y'],\n",
       " ['requires_job_training', 'N'],\n",
       " ['no_of_employees', 183],\n",
       " ['yr_of_estab', 1998],\n",
       " ['region_of_employment', 'Northeast'],\n",
       " ['prevailing_wage', 138.76],\n",
       " ['unit_of_wage', 'Year'],\n",
       " ['full_time_position', 'Y'],\n",
       " ['case_status', 'Certified']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i,df.groupby(i).count().orderBy(\"count\", ascending=False).first()[0]] for i in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2b27d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|case_id|continent|education_of_employee|has_job_experience|requires_job_training|no_of_employees|yr_of_estab|region_of_employment|prevailing_wage|unit_of_wage|full_time_position|case_status|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "|      0|        0|                    0|                 0|                    0|              0|          0|                   0|              0|           0|                 0|          0|\n",
      "+-------+---------+---------------------+------------------+---------------------+---------------+-----------+--------------------+---------------+------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "490636c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(case_status='Denied', label=1.0),\n",
       " Row(case_status='Certified', label=0.0),\n",
       " Row(case_status='Denied', label=1.0),\n",
       " Row(case_status='Denied', label=1.0),\n",
       " Row(case_status='Certified', label=0.0),\n",
       " Row(case_status='Certified', label=0.0),\n",
       " Row(case_status='Certified', label=0.0),\n",
       " Row(case_status='Denied', label=1.0),\n",
       " Row(case_status='Certified', label=0.0),\n",
       " Row(case_status='Certified', label=0.0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_To_integer = StringIndexer(\n",
    "                                   inputCol = \"case_status\",\n",
    "                                   outputCol = \"label\"\n",
    "                                 )\n",
    "\n",
    "# 6.4.1\n",
    "model = label_To_integer.fit(df.select(\"case_status\"))\n",
    "model.transform(df.select('case_status')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44b46e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ee3e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "                stages =  [\n",
    "                            # StringIndex target\n",
    "                            StringIndexer(\n",
    "                                           inputCol =\"case_status\",\n",
    "                                           outputCol = \"label\"\n",
    "                                           ),\n",
    "\n",
    "                             # StringIndex all categorical cols\n",
    "                             StringIndexer(\n",
    "                                          inputCols = cat_cols,\n",
    "                                          outputCols = cat_cols_si\n",
    "                                         ),\n",
    "                    \n",
    "                             # OHE, StringIndexed columns\n",
    "                             OneHotEncoder(\n",
    "                                           inputCols = cat_cols_si,\n",
    "                                           outputCols = ohe_cols\n",
    "                                          ),\n",
    "                    \n",
    "                              # Assemble OHE + num_cols\n",
    "                              VectorAssembler\n",
    "                                           (\n",
    "                                           inputCols = ohe_cols + num_cols,\n",
    "                                           outputCol = 'ass_features'    \n",
    "\n",
    "                                           ),\n",
    "                             \n",
    "                             #  Scale 'vector' column\n",
    "                             StandardScaler(\n",
    "                                            inputCol = 'ass_features',\n",
    "                                            outputCol = 'features'\n",
    "\n",
    "                                           )\n",
    "                     \n",
    "                       ]\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f34dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 07:30:28,899 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "2023-10-14 07:30:29,311 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model= pipe.fit(df)\n",
    "\n",
    "df_trans = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba6a8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_id', 'continent', 'education_of_employee', 'has_job_experience', 'requires_job_training', 'no_of_employees', 'yr_of_estab', 'region_of_employment', 'prevailing_wage', 'unit_of_wage', 'full_time_position', 'case_status', 'label', 'case_id_index', 'continent_index', 'education_of_employee_index', 'has_job_experience_index', 'requires_job_training_index', 'region_of_employment_index', 'unit_of_wage_index', 'full_time_position_index', 'case_status_index', 'case_id_ohe', 'continent_ohe', 'education_of_employee_ohe', 'has_job_experience_ohe', 'requires_job_training_ohe', 'region_of_employment_ohe', 'unit_of_wage_ohe', 'full_time_position_ohe', 'case_status_ohe', 'ass_features', 'features']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(df_trans.columns)\n",
    "\n",
    "\n",
    "len(df.columns)         \n",
    "len(df_trans.columns)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85e2555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 In building Pipeline,\n",
    "#      keyword 'stages' is a must\n",
    "\n",
    "pipe = Pipeline(\n",
    "                stages =  [\n",
    "                            # 11.0.1 StringIndex target\n",
    "                            StringIndexer(\n",
    "                                           inputCol =\"income\",\n",
    "                                           handleInvalid = 'keep',\n",
    "                                           outputCol = \"label\"\n",
    "                                           ),\n",
    "\n",
    "                             # 11.0.2 StringIndex all categorical cols\n",
    "                             StringIndexer(\n",
    "                                          inputCols = cat_cols,\n",
    "                                          handleInvalid = 'keep',\n",
    "                                          outputCols = cat_cols_si\n",
    "                                         ),\n",
    "                    \n",
    "                             # 11.0.3 OHE, StringIndexed columns\n",
    "                             OneHotEncoder(\n",
    "                                           inputCols = cat_cols_si,\n",
    "                                           outputCols = ohe_cols\n",
    "                                          ),\n",
    "                    \n",
    "                              # 11.0.4 Assemble OHE + num_cols\n",
    "                              VectorAssembler\n",
    "                                           (\n",
    "                                           inputCols = ohe_cols + num_cols,\n",
    "                                           outputCol = 'ass_features'    \n",
    "\n",
    "                                           ),\n",
    "                             \n",
    "                             # 11.0.5 Scale 'vector' column\n",
    "                             StandardScaler(\n",
    "                                            inputCol = 'ass_features',\n",
    "                                            outputCol = 'features'\n",
    "                                           ),\n",
    "                            \n",
    "                             # 11.0.6 Modeling\n",
    "                             # https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\n",
    "                             LogisticRegression(\n",
    "                                 \n",
    "                                                 featuresCol='features',\n",
    "                                                 labelCol='label',\n",
    "                                                 maxIter=30\n",
    "                                               )\n",
    "                    \n",
    "                       ]\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41b2ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "174f53c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20272"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd39a459",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1845.fit.\n: org.apache.spark.SparkException: Input column income does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m lrModel \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m (end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py:339\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 339\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py:336\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1845.fit.\n: org.apache.spark.SparkException: Input column income does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lrModel = pipe.fit(train)\n",
    "end = time.time()\n",
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9edac9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['case_id', 'continent', 'education_of_employee', 'has_job_experience', 'requires_job_training', 'no_of_employees', 'yr_of_estab', 'region_of_employment', 'prevailing_wage', 'unit_of_wage', 'full_time_position', 'case_status', 'label', 'case_id_index', 'continent_index', 'education_of_employee_index', 'has_job_experience_index', 'requires_job_training_index', 'region_of_employment_index', 'unit_of_wage_index', 'full_time_position_index', 'case_status_index', 'case_id_ohe', 'continent_ohe', 'education_of_employee_ohe', 'has_job_experience_ohe', 'requires_job_training_ohe', 'region_of_employment_ohe', 'unit_of_wage_ohe', 'full_time_position_ohe', 'case_status_ohe', 'ass_features', 'features']\n",
      "\n",
      "root\n",
      " |-- case_id: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- education_of_employee: string (nullable = true)\n",
      " |-- has_job_experience: string (nullable = true)\n",
      " |-- requires_job_training: string (nullable = true)\n",
      " |-- no_of_employees: integer (nullable = true)\n",
      " |-- yr_of_estab: integer (nullable = true)\n",
      " |-- region_of_employment: string (nullable = true)\n",
      " |-- prevailing_wage: double (nullable = true)\n",
      " |-- unit_of_wage: string (nullable = true)\n",
      " |-- full_time_position: string (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- case_id_index: double (nullable = false)\n",
      " |-- continent_index: double (nullable = false)\n",
      " |-- education_of_employee_index: double (nullable = false)\n",
      " |-- has_job_experience_index: double (nullable = false)\n",
      " |-- requires_job_training_index: double (nullable = false)\n",
      " |-- region_of_employment_index: double (nullable = false)\n",
      " |-- unit_of_wage_index: double (nullable = false)\n",
      " |-- full_time_position_index: double (nullable = false)\n",
      " |-- case_status_index: double (nullable = false)\n",
      " |-- case_id_ohe: vector (nullable = true)\n",
      " |-- continent_ohe: vector (nullable = true)\n",
      " |-- education_of_employee_ohe: vector (nullable = true)\n",
      " |-- has_job_experience_ohe: vector (nullable = true)\n",
      " |-- requires_job_training_ohe: vector (nullable = true)\n",
      " |-- region_of_employment_ohe: vector (nullable = true)\n",
      " |-- unit_of_wage_ohe: vector (nullable = true)\n",
      " |-- full_time_position_ohe: vector (nullable = true)\n",
      " |-- case_status_ohe: vector (nullable = true)\n",
      " |-- ass_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PREDICTIONS\n",
    "\n",
    "\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "\n",
    "print()\n",
    "print(predictions.columns)      \n",
    "\n",
    "\n",
    "print()\n",
    "predictions.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fad196fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+---------------+-----------+\n",
      "|label|continent|unit_of_wage|prevailing_wage|case_status|\n",
      "+-----+---------+------------+---------------+-----------+\n",
      "|  0.0|     Asia|        Year|       53635.39|  Certified|\n",
      "|  0.0|     Asia|        Hour|       849.9988|  Certified|\n",
      "|  0.0|     Asia|        Year|       14130.76|  Certified|\n",
      "+-----+---------+------------+---------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"continent\", \"unit_of_wage\", \"prevailing_wage\", \"case_status\")\n",
    "selected.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53d82291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "2023-10-14 07:40:16,269 ERROR executor.Executor: Exception in task 0.0 in stage 139.0 (TID 98)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n",
      "    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n",
      "TypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2023-10-14 07:40:16,273 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 139.0 (TID 98) (master executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n",
      "    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n",
      "TypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2023-10-14 07:40:16,273 ERROR scheduler.TaskSetManager: Task 0 in stage 139.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8405.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 139.0 failed 1 times, most recent failure: Lost task 0.0 in stage 139.0 (TID 98) (master executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_status\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MulticlassMetrics(preds_and_labels\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mtuple\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusionMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray())\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:289\u001b[0m, in \u001b[0;36mMulticlassMetrics.confusionMatrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusionMatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    Returns confusion matrix: predicted classes are in columns,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    they are ordered by class label ascending, as in \"labels\".\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusionMatrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/mllib/common.py:141\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[38;5;241m*\u001b[39ma):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/mllib/common.py:118\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\" Call Java Function \"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark-3.2.3-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8405.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 139.0 failed 1 times, most recent failure: Lost task 0.0 in stage 139.0 (TID 98) (master executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1392, in verify_struct\n    verifier(v)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1405, in verify_default\n    verify_acceptable_types(obj)\n  File \"/opt/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/types.py\", line 1293, in verify_acceptable_types\n    raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\nTypeError: field prediction: DoubleType can not accept object 'Certified' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "preds_and_labels = predictions.select(['case_status','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "678b2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "#     Ref: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder\n",
    "#     Ref: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
    "\n",
    "#  K-fold cross validation performs model selection by splitting the dataset into a set of\n",
    "#    non-overlapping randomly partitioned folds which are used as separate training and\n",
    "#     test datasets e.g., with k=3 folds, K-fold cross validation will generate 3 (training, test)\n",
    "#      dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing.\n",
    "#       Each fold is used as the test set exactly once.\n",
    "\n",
    "# One Way\n",
    "\n",
    "lr = pipe.getStages()[5]\n",
    "\n",
    "\n",
    "\n",
    "grid = (\n",
    "        ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [ 0.01, 1, 2.0]) \\\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "                 .addGrid(lr.maxIter, [1, 10]) \\\n",
    "                 .build()\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c5dea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# And crossvalidator\n",
    "cv = CrossValidator(estimator= pipe, \\\n",
    "                    estimatorParamMaps= grid,     \\\n",
    "                    evaluator=evaluator,  \\\n",
    "                    numFolds=3\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "087888e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CrossValidator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (((end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m) , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CrossValidator' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "cvModel = cv.fit(train)\n",
    "end = time.time()\n",
    "print (((end-start)/60) , \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f00a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
